Resilient Distributed Datasets
[ 30 ]
We first create a list of all the values of the rdd1 using the .map(...) transformation,
and then use the .reduce(...) method to process the results. The reduce(...)
method, on each partition, runs the summation method (here expressed as a lambda)
and returns the sum to the driver node where the final aggregation takes place.
A word of caution is necessary here. The functions passed as a reducer
need to be associative, that is, when the order of elements is changed the
result does not, and commutative, that is, changing the order of operands
does not change the result either.
The example of the associativity rule is (5 + 2) + 3 = 5 + (2 + 3), and of the
commutative is 5 + 2 + 3 = 3 + 2 + 5. Thus, you need to be careful about
what functions you pass to the reducer.
If you ignore the preceding rule, you might run into trouble (assuming
your code runs at all). For example, let's assume we have the following
RDD (with one partition only!):
data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)
If we were to reduce the data in a manner that we would like to divide the
current result by the subsequent one, we would expect a value of 10:
works = data_reduce.reduce(lambda x, y: x / y)
However, if you were to partition the data into three partitions, the result
will be wrong:
data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3)
data_reduce.reduce(lambda x, y: x / y)
It will produce 0.004.
The .reduceByKey(...) method works in a similar way to the .reduce(...)
method, but it performs a reduction on a key-by-key basis:
data_key = sc.parallelize(
[('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),
('d', 3)],4)
data_key.reduceByKey(lambda x, y: x + y).collect()
The preceding code produces the following: