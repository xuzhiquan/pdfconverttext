Chapter 3
[ 49 ]
An important note when working with Spark SQL and DataFrames is that
while it is easy to work with CSV, JSON, and a variety of data formats,
the most common storage format for Spark SQL analytics queries is the
Parquet file format. It is a columnar format that is supported by many
other data processing systems and Spark SQL supports both reading
and writing Parquet files that automatically preserves the schema of
the original data. For more information, please refer to the latest Spark
SQL Programming Guide > Parquet Files at: http://spark.apache.
org/docs/latest/sql-programming-guide.html#parquet-
files. Also, there are many performance optimizations that pertain
to Parquet, including (but not limited to) Automatic Partition Discovery
and Schema Migration for Parquet at https://databricks.com/
blog/2015/03/24/spark-sql-graduates-from-alpha-in-
spark-1-3.html and How Apache Spark performs a fast count using the
parquet metadata at https://github.com/dennyglee/databricks/
blob/master/misc/parquet-count-metadata-explanation.md.
DataFrame scenario â€“ on-time flight
performance
To showcase the types of queries you can do with DataFrames, let's look at the use
case of on-time flight performance. We will analyze the Airline On-Time Performance
and Causes of Flight Delays: On-Time Data (http://bit.ly/2ccJPPM), and join this
with the airports dataset, obtained from the Open Flights Airport, airline, and route
data (http://bit.ly/2ccK5hw), to better understand the variables associated with
flight delays.
For this section, we will be using Databricks Community Edition (a free
offering of the Databricks product), which you can get at https://
databricks.com/try-databricks. We will be using visualizations
and pre-loaded datasets within Databricks to make it easier for you to
focus on writing the code and analyzing the results.
If you would prefer to run this on your own environment, you can
find the datasets available in our GitHub repository for this book at
https://github.com/drabastomek/learningPySpark.