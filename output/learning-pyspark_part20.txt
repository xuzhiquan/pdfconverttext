Preface
[ ix ]
What you need for this book
For this book you need a personal computer (can be either Windows machine,
Mac, or Linux). To run Apache Spark, you will need Java 7+ and an installed and
configured Python 2.6+ or 3.4+ environment; we use the Anaconda distribution of
Python in version 3.5, which can be downloaded from https://www.continuum.
io/downloads.
The Python modules we randomly use throughout the book come preinstalled
with Anaconda. We also use GraphFrames and TensorFrames that can be loaded
dynamically while starting a Spark instance: to load these you just need an Internet
connection. It is fine if some of those modules are not currently installed on your
machine â€“ we will guide you through the installation process.
Who this book is for
This book is for everyone who wants to learn the fastest-growing technology in big
data: Apache Spark. We hope that even the more advanced practitioners from the
field of data science can find some of the examples refreshing and the more advanced
topics interesting.
Conventions
In this book, you will find a number of styles of text that distinguish between
different kinds of information. Here are some examples of these styles, and an
explanation of their meaning.
Code words in text, database table names, folder names, filenames, file extensions,
pathnames, dummy URLs, user input, and Twitter handles are shown as follows:
A block of code is set as follows:
data = sc.parallelize(
[('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12),
('Amber', 9)])
When we wish to draw your attention to a particular part of a code block, the
relevant lines or items are set in bold:
rdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',10)])
rdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', '6'), ('d', 15)])
rdd3 = rdd1.leftOuterJoin(rdd2)
https://www.iteblog.com